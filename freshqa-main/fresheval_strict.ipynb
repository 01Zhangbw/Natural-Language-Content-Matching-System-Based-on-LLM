{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cIMEmj9Esik",
    "outputId": "b8953f3b-8ff6-42b5-ff15-af0aaf05406d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\d\\python\\lib\\site-packages (1.30.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\d\\python\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\d\\python\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\d\\python\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\d\\python\\lib\\site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in d:\\d\\python\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\d\\python\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\d\\python\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\d\\python\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\d\\python\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\d\\python\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\d\\python\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\d\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\d\\python\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: colorama in d:\\d\\python\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#@title Installing required Python packages\n",
    "\n",
    "\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gspread in d:\\d\\python\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in d:\\d\\python\\lib\\site-packages (from gspread) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in d:\\d\\python\\lib\\site-packages (from gspread) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\d\\python\\lib\\site-packages (from google-auth>=1.12.0->gspread) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\d\\python\\lib\\site-packages (from google-auth>=1.12.0->gspread) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\d\\python\\lib\\site-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\d\\python\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\d\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\d\\python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\d\\python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\d\\python\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\d\\python\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\d\\python\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\d\\python\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "wW3P9qyxHHU8"
   },
   "outputs": [],
   "source": [
    "#@title Importing Python libraries and modules\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gspread\n",
    "from google.auth import default\n",
    "# from google.colab import auth\n",
    "# from google.colab import files\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from openai import OpenAI\n",
    "import tabulate\n",
    "import textwrap\n",
    "\n",
    "\n",
    "current_date = datetime.datetime.now(\n",
    "        pytz.timezone(\"America/Los_Angeles\")\n",
    "    ).strftime(\"%B %d, %Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "98YRRHnz0SGu"
   },
   "outputs": [],
   "source": [
    "#@title API keys\n",
    "\n",
    "\n",
    "# OpenAI's API key (sign up at https://platform.openai.com/signup to get $5 in\n",
    "# free credit that can be used during your first 3 months)\n",
    "openai_api_key = \"sk-JnWHmZFfrx9mWDahm7pJhfDoQNON5zDtm4jabuapWyp09yll\"  # @param {type:\"string\"}\n",
    "openai_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")\n",
    "\n",
    "assert (\n",
    "      openai_api_key is not None and openai_api_key != ''\n",
    "  ), \"OpenAI's API key is not set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "7x4S8-FHHtK1"
   },
   "outputs": [],
   "source": [
    "#@title 调用基础LLM的函数\n",
    "\n",
    "def call_llm_api(prompt, model, temperature, max_tokens, chat_completions=True):\n",
    "  # 参见 https://platform.openai.com/docs/guides/gpt 获取详细信息\n",
    "  if chat_completions:\n",
    "    # 使用聊天完成API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,                 # 指定使用的模型\n",
    "        temperature=temperature,     # 设置生成文本的温度\n",
    "        max_tokens=max_tokens,       # 设置生成文本的最大长度\n",
    "        messages=[                   # 消息列表，包含系统、用户和助手的消息\n",
    "            {\n",
    "                \"role\": \"system\",    # 系统角色的消息\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant. Respond as concisely as\"\n",
    "                    f\" possible. Knowledge cutoff: {current_date}.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What's today's date?\"},   # 用户角色的消息\n",
    "            {\n",
    "                \"role\": \"assistant\", # 助手角色的消息\n",
    "                \"content\": f\"Today is {current_date} in Pacific Standard Time.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},  # 用户输入的提示\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content   # 返回助手生成的回复内容\n",
    "\n",
    "  else:\n",
    "    # 使用完成API\n",
    "    response = openai_client.completions.create(\n",
    "        model=model,                 # 指定使用的模型\n",
    "        temperature=temperature,     # 设置生成文本的温度\n",
    "        max_tokens=max_tokens,       # 设置生成文本的最大长度\n",
    "        prompt=prompt,               # 用户输入的提示\n",
    "    )\n",
    "    return response.choices[0].text   # 返回生成的文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xmQZYfPD3sxL"
   },
   "outputs": [],
   "source": [
    "#@title Instructions & demonstration examples\n",
    "\n",
    "# prompt的内容\n",
    "prefix = (\n",
    "    \"请对一个问题的回答进行严格评估，其中不允许有虚构信息、过时信息或不完整的答案。\"\n",
    "    \"只有在提供一个自信且明确的答案，或可以从回答中明显推断出正确答案的情况下，才能给予评分。\"\n",
    "    \"主要或最终答案单独存在时必须准确。任何附加信息不得与主要答案相矛盾或改变人们对其的理解。\"\n",
    "    \"对于带有错误前提的问题，回答必须指出错误前提的存在，才能获得评分。对于涉及实体名称（如人名）的回答，\"\n",
    "    \"要求使用完整名称或公认的名称。关于数值回答，除非在标准答案中明确包括，大致数值通常不被接受。\"\n",
    "    \"回答中如果包含任何虚构信息，无论多么轻微，都不会获得评分。此外，当回答表明信息可能过时时，\"\n",
    "    \"只有在明显知识未改变（例如通过常识或众所周知的事实）时，我们才接受。\"\n",
    ")\n",
    "# 代码的示例\n",
    "demo_examples = [\n",
    "    {\n",
    "        \"question\": \"How old is the world's oldest verified living person?\",\n",
    "        \"correct_answers\": [\n",
    "            \"117 years old\",\n",
    "            \"117\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            f\"As of today {current_date}, the most up-to-date and relevant\"\n",
    "            \" information regarding this query is as follows. The oldest\"\n",
    "            \" verified living person is Maria Branyas Morera, who was born\"\n",
    "            \" on March 4, 1907, making her 117 years old.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response\"\n",
    "            \" (117 years old) is accurate, and all the provided information\"\n",
    "            \" is accurate and up-to-date. Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When did the UK adopt the Euro?\",\n",
    "        \"correct_answers\": [\"The United Kingdom has never adopted the Euro.\"],\n",
    "        \"response\": (\n",
    "            \"The UK has never adopted the Euro as its official currency. The\"\n",
    "            \" country has retained the British pound sterling (GBP) as its\"\n",
    "            \" currency.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response debunks the\"\n",
    "            \" question's false premise (i.e., it points out that the UK has\"\n",
    "            \" never adopted the Euro), and all the provided information is\"\n",
    "            \" accurate and up-to-date. Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"How much longer will Brittney Griner spend in the Russian prison?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\n",
    "            \"She was released in December 2022 as part of a prisoner swap.\"\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"I'm sorry, but I have no information to suggest that Brittney\"\n",
    "            \" Griner is currently in a Russian prison. As far as I know, she\"\n",
    "            \" is a professional basketball player who currently plays for the\"\n",
    "            \" Phoenix Mercury in the Women's National Basketball Association\"\n",
    "            \" (WNBA). If you have any more information or context, please let\"\n",
    "            \" me know and I'll do my best to help.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response fails to\"\n",
    "            \" debunk the question's false premise by not explicitly stating\"\n",
    "            \" that Brittney Griner was already released. Additionally, the\"\n",
    "            \" response lacks a confident and definitive answer. Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Which language has the third largest population by number of\"\n",
    "            \" native speakers?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"English\"],\n",
    "        \"response\": \"1. Mandarin 2. Spanish 3. English\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the correct answer (English) can\"\n",
    "            \" be inferred from the response, the response lacks an explicit\"\n",
    "            \" statement, which is not desirable.\"\n",
    "            \" Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Has Virginia Woolf's novel about the Ramsay family entered the\"\n",
    "            \" public domain in the United States?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Yes\"],\n",
    "        \"response\": (\n",
    "            \"1927's To the Lighthouse is in the public domain in the United\"\n",
    "            \" States. The final answer is To the Lighthouse.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary information in the\"\n",
    "            \" response (1927's To the Lighthouse is in the public domain in\"\n",
    "            \" the United States) is accurate, the final answer when standing\"\n",
    "            ' alone is inaccurate (it should be \"Yes\" instead of \"To the'\n",
    "            ' Lighthouse\"). Thus, the response is not credited.'\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is Tesla's stock price above $250 a share?\",\n",
    "        \"correct_answers\": [\"No\"],\n",
    "        \"response\": \"No, it isn't. The stock price is currently at $257.\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (No) is accurate, the additional information\"\n",
    "            \" contradicts the primary answer ($257 is above $250). Thus,\"\n",
    "            \" the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When did Lionel Messi win his second FIFA World Cup?\",\n",
    "        \"correct_answers\": [ # 根据搜索引擎进行的输出\n",
    "            \"Messi has not won his second FIFA World Cup.\",\n",
    "            \"Messi has won only one FIFA World Cup.\",\n",
    "        ],\n",
    "        \"response\": ( # 根据大模型进行的输出\n",
    "            \"This question does not contain a valid premise. Lionel Messi has\"\n",
    "            \" never won a FIFA World Cup.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. While the response\"\n",
    "            \" debunks the question's false premise (i.e., it suggests that\"\n",
    "            \" Messi has not won his second FIFA World Cup), it inaccurately\"\n",
    "            \" shapes one's knowledge perception of the primary answer, as\"\n",
    "            \" Messi has already won one FIFA World Cup. Thus,\"\n",
    "            \" the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"How many times has Jeremy Corbyn served as Prime Minister of\"\n",
    "            \" the UK?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\n",
    "            \"Corbyn has never served as Prime Minister of the UK.\"\n",
    "        ],\n",
    "        \"response\": \"Zero\",\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response lacks a\"\n",
    "            \" rebuttal, which is always required for false-premise questions.\"\n",
    "            \" Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who founded Amazon?\",\n",
    "        \"correct_answers\": [\"Jeff Bezos\", \"Jeffrey Preston Bezos\"],\n",
    "        \"response\": \"100% of people said Jeff.\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. Given that the question involves a\"\n",
    "            \" person's name, a complete name or commonly recognized name is\"\n",
    "            ' expected (\"Jeff Bezos\" instead of \"Jeff\"). Thus, the response'\n",
    "            \" is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many books has Colleen Hoover published?\",\n",
    "        \"correct_answers\": [\"26 books\", \"26\"],\n",
    "        \"response\": (\n",
    "            f\"As of today {current_date}, the most up-to-date and relevant\"\n",
    "            \" information regarding this query is as follows. Colleen Hoover\"\n",
    "            \" has published over 20 books and novellas.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. For a numerical answer, an approximate\"\n",
    "            \" value (over 20 books) is generally not accepted unless\"\n",
    "            \" explicitly included in the correct answers. Thus, the response\"\n",
    "            \" is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the largest model size of GPT-3?\",\n",
    "        \"correct_answers\": [\n",
    "            \"175 billion parameters\",\n",
    "            \"175B parameters\",\n",
    "            \"175 billion\",\n",
    "            \"175B\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"175 billion parameters. It is a 175 billion parameter, 1.37TB,\"\n",
    "            \" 137B token, 137B wordpiece, 137B BPE, 137B byte pair encoding,\"\n",
    "            \" 137B BPE token, 137B BPE wordpiece, 137B BPE token, 137B BPE\"\n",
    "            \" wordpiece\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the response\"\n",
    "            \" (175 billion parameters) is accurate, the response is ill-formed\"\n",
    "            \" and contains repetitions. Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many member nations exist in the United Nations?\",\n",
    "        \"correct_answers\": [\n",
    "            \"193\",\n",
    "            \"193 members\",\n",
    "            \"193 member states\",\n",
    "            \"193 sovereign states\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"As of my last update in April 2023, there were 193\"\n",
    "            \" member nations in the United Nations. Please note that this\"\n",
    "            \" information may have changed since then, so it is advisable to\"\n",
    "            \" check the latest information from the United Nations or other\"\n",
    "            \" reliable sources.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response (193\"\n",
    "            \" member nations) is accurate. However, the response fails to\"\n",
    "            \" provide a current answer and instead relies on outdated \"\n",
    "            \" information without confirming its continued accuracy. Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who won the biggest single-tournament payday in tennis history?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Novak Djokovic\"],\n",
    "        \"response\": (\n",
    "            \"The biggest single-tournament payday in tennis history was won\"\n",
    "            \" by Novak Djokovic at the 2021 Australian Open. He received a\"\n",
    "            \" prize money of AUD 2.75 million (approximately USD 2.1 million)\"\n",
    "            \" for winning the men's singles title.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (Novak Djokovic) is accurate, the additional provided\"\n",
    "            \" information is outdated or hallucinated (by winning the 2022\"\n",
    "            \" ATP Finals title, Djokovic earned a record $4,740,300, the\"\n",
    "            \" highest payout for a tournament winner in tennis). Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who was the first football player to score one hundred\"\n",
    "            \" international goals?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Ali Daei\"],\n",
    "        \"response\": (\n",
    "            \"As of my last update in April 2023, Ali Daei, a\"\n",
    "            \" former Iranian footballer, was the first player to score 100\"\n",
    "            \" international goals.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response\"\n",
    "            \" (Ali Daei) is accurate. Although the knowledge cutoff date is\"\n",
    "            \" in 2021, this fact is timeless because Ali Daei was the initial\"\n",
    "            \" achiever of this milestone, and he will always remain the first).\"\n",
    "            \" Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who is the latest winner of the Formula 1 world championship?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Max Verstappen\", \"Max Emilian Verstappen\"],\n",
    "        \"response\": (\n",
    "            \"Max Verstappen won the 2023 Formula 1 world championship. He is\"\n",
    "            \" a Dutch racing driver who competes in Formula One for Red Bull\"\n",
    "            \" Racing. He is the son of former Formula One driver Jos\"\n",
    "            \" Verstappen. Verstappen started his racing career in karting at\"\n",
    "            \" the age of seven. He won the 2013 European Karting Championship\"\n",
    "            \" and the 2013 World Karting Championship. In 2014, he moved to\"\n",
    "            \" single-seater racing, competing in the European Formula Three\"\n",
    "            \" Championship. He won the championship in his first season,\"\n",
    "            \" becoming the first driver to do so since Nico Rosberg in 2005.\"\n",
    "            \" In 2015, Verstappen moved to Formula One, driving for Toro\"\n",
    "            \" Rosso. He became the youngest driver to compete in Formula One\"\n",
    "            \" at the age of 17. He scored his first points in Formula One at\"\n",
    "            \" the 2015 Hungarian GrandPrix. In 2016, Verstappen moved to Red\"\n",
    "            \" Bull Racing. He won his first race at the 2016 Spanish Grand\"\n",
    "            \" Prix. He became the youngest driver to win a race in Formula\"\n",
    "            \" One at theage of 18. Verstappen finished the 2016 season in\"\n",
    "            \" third place in the drivers' championship. In 2017, Verstappen\"\n",
    "            \" won four races and finished the season in second place in the\"\n",
    "            \" drivers' championship. In 2018, Verstappen won seven races and\"\n",
    "            \" finished the season in second place in the drivers'\"\n",
    "            \" championship. In 2019, Verstappen won nine races and finished\"\n",
    "            \" the season in first place in the drivers' championship. He is\"\n",
    "            \" the first Dutch driver to win the Formula One world\"\n",
    "            \" championship.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (Max Verstappen) is accurate, the response contains\"\n",
    "            \" several instances of hallucinated information (e.g., Max\"\n",
    "            \" Verstappen did not win the Formula Three European Championship\"\n",
    "            \" in 2014). Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "]\n",
    "\n",
    "demo_questions = [ex[\"question\"] for ex in demo_examples]\n",
    "\n",
    "demo_evaluation_template = (\n",
    "    \"\\ncorrect answer(s): {correct_answers}\"\n",
    "    \"\\nresponse: {response}\"\n",
    "    \"\\ncomment: {comment}\"\n",
    "    \"\\nevaluation: {evaluation}\"\n",
    ")\n",
    "evaluation_template = (\n",
    "    \"\\ncorrect answer(s): {correct_answers}\"\n",
    "    \"\\nresponse: {response}\"\n",
    "    \"\\ncomment: \"\n",
    ")\n",
    "\n",
    "demo_evaluations = []\n",
    "for ex in demo_examples:\n",
    "  demo_evaluation = demo_evaluation_template.format(\n",
    "      question=ex[\"question\"],\n",
    "      correct_answers=' | '.join(ex[\"correct_answers\"]),\n",
    "      response=ex[\"response\"],\n",
    "      comment=ex[\"comment\"],\n",
    "      evaluation=ex[\"evaluation\"],\n",
    "  )\n",
    "  demo_evaluations.append(demo_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "fAcuImw5EP5T"
   },
   "outputs": [],
   "source": [
    "#@title Function calling for FreshEval\n",
    "\n",
    "\n",
    "def call_fresheval(model, prefix, question, response, correct_answers,\n",
    "                   evaluation):\n",
    "  temperature = 0.0\n",
    "  max_tokens = 256\n",
    "  chat_completions = True\n",
    "\n",
    "  if model.startswith('gpt-4'):\n",
    "    num_organic_results = 15\n",
    "    num_related_questions = 3\n",
    "    num_questions_and_answers = 3\n",
    "    num_retrieved_evidences = 15\n",
    "  else:\n",
    "    num_organic_results = 15\n",
    "    num_related_questions = 2\n",
    "    num_questions_and_answers = 2\n",
    "    num_retrieved_evidences = 5\n",
    "\n",
    "  # Generate prompts for demo examples\n",
    "  demo_prompts = []\n",
    "  for q, e in zip(demo_questions, demo_evaluations):\n",
    "      demo_prompts.append(f'\\n\\n\\nquestion: {q}{e}')\n",
    "\n",
    "  fresheval_demo = ''.join(demo_prompts).strip()\n",
    "\n",
    "  fresheval_question = f'\\n\\n\\nquestion: {question}{evaluation}'\n",
    "\n",
    "  fresh_eval = prefix + '\\n\\n\\n' + fresheval_demo + fresheval_question\n",
    "  answer = call_llm_api(\n",
    "      fresh_eval, model, temperature, max_tokens, chat_completions\n",
    "  )\n",
    "\n",
    "  return answer\n",
    "\n",
    "\n",
    "def extract_ratings(response):\n",
    "  evaluation = None\n",
    "  for line in response.split('\\n'):\n",
    "    if 'evaluation: ' in line:\n",
    "      evaluation = line.split(' ')[-1]\n",
    "      if evaluation not in ['correct', 'incorrect']:\n",
    "        return False, {'rating': None}\n",
    "      if evaluation == 'incorrect':\n",
    "        evaluation = 'FALSE'\n",
    "      else:\n",
    "        evaluation = 'TRUE'\n",
    "  if evaluation is None:\n",
    "    if 'Thus, the response is credited.' in response:\n",
    "      evaluation = 'TRUE'\n",
    "    elif 'Thus, the response is not credited.' in response:\n",
    "      evaluation = 'FALSE'\n",
    "    else:\n",
    "      return False, {'rating': None}\n",
    "  return True, {'rating': evaluation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDrive in d:\\d\\python\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in d:\\d\\python\\lib\\site-packages (from PyDrive) (2.130.0)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in d:\\d\\python\\lib\\site-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in d:\\d\\python\\lib\\site-packages (from PyDrive) (6.0.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in d:\\d\\python\\lib\\site-packages (from google-api-python-client>=1.2->PyDrive) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in d:\\d\\python\\lib\\site-packages (from google-api-python-client>=1.2->PyDrive) (2.29.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\d\\python\\lib\\site-packages (from google-api-python-client>=1.2->PyDrive) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in d:\\d\\python\\lib\\site-packages (from google-api-python-client>=1.2->PyDrive) (2.19.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\d\\python\\lib\\site-packages (from google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in d:\\d\\python\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in d:\\d\\python\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in d:\\d\\python\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in d:\\d\\python\\lib\\site-packages (from oauth2client>=4.0.0->PyDrive) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\d\\python\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in d:\\d\\python\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\d\\python\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.23.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\d\\python\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\d\\python\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.2->PyDrive) (5.3.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\d\\python\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.2->PyDrive) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\d\\python\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\d\\python\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\d\\python\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\d\\python\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-colab==1.0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/199.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/199.4 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 30.7/199.4 kB 217.9 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 30.7/199.4 kB 217.9 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 30.7/199.4 kB 217.9 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 71.7/199.4 kB 261.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 153.6/199.4 kB 482.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 174.1/199.4 kB 523.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 174.1/199.4 kB 523.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 174.1/199.4 kB 523.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 174.1/199.4 kB 523.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 199.4/199.4 kB 366.6 kB/s eta 0:00:00\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件编码：Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# 打开文件并读取内容\n",
    "with open('fresheval_strict_sample_evaluation_spreadsheet - freshqa.csv', 'rb') as f:\n",
    "    rawdata = f.read()\n",
    "\n",
    "# 检测文件编码\n",
    "result = chardet.detect(rawdata)\n",
    "file_encoding = result['encoding']\n",
    "\n",
    "print(f\"文件编码：{file_encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "SMf6R0VHfzx-",
    "outputId": "3c65fb2e-a67a-4edd-95d1-0d686d25c45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating example id=0...\n",
      "Done\n",
      "Evaluating example id=1...\n",
      "Done\n",
      "Evaluating example id=2...\n",
      "Invalid evaluation, reevaluating...\n",
      "Done\n",
      "Evaluating example id=3...\n",
      "Done\n",
      "Evaluating example id=4...\n",
      "Done\n",
      "Evaluating example id=5...\n",
      "Invalid evaluation, reevaluating...\n",
      "Done\n",
      "Evaluating example id=6...\n",
      "Done\n",
      "Evaluating example id=7...\n",
      "Invalid evaluation, reevaluating...\n",
      "Invalid evaluation, reevaluating...\n",
      "Done\n",
      "Evaluating example id=8...\n",
      "Done\n",
      "Evaluating example id=9...\n",
      "Invalid evaluation, reevaluating...\n",
      "Invalid evaluation, reevaluating...\n",
      "Done\n",
      "评估结果已保存到 fresheval_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\" #@param [\"gpt-4-0125-preview\", \"gpt-4-turbo-preview\", \"gpt-4-1106-preview\", \"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-instruct\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0301\"]\n",
    "input_csv_file = \"fresheval_strict_sample_evaluation_spreadsheet - freshqa_2.csv\" # 本地输入CSV文件\n",
    "output_csv_file = \"fresheval_output.csv\" # 本地输出CSV文件\n",
    "num_evals = 10\n",
    "\n",
    "# 读取本地CSV文件\n",
    "df = pd.read_csv(input_csv_file,encoding=\"Windows-1252\")\n",
    "df = df[:num_evals]\n",
    "\n",
    "freshevals = []\n",
    "for index, row in df.iterrows():\n",
    "    print(f'Evaluating example id={index}...')\n",
    "    question = row['question']\n",
    "    response = row['model_response']\n",
    "    correct_answers = [row[f'answer_{i}'] for i in range(10) if f'answer_{i}' in row]\n",
    "    correct_answers = [x for x in correct_answers if pd.notna(x)]\n",
    "\n",
    "    evaluation = evaluation_template.format(\n",
    "        correct_answers=' | '.join(correct_answers),\n",
    "        response=response,\n",
    "    )\n",
    "\n",
    "    fresheval = call_fresheval(\n",
    "        model_name,\n",
    "        prefix,\n",
    "        question,\n",
    "        response,\n",
    "        correct_answers,\n",
    "        evaluation,\n",
    "    )\n",
    "    is_valid_eval, eval = extract_ratings(fresheval)\n",
    "    if is_valid_eval:\n",
    "        print('Done')\n",
    "\n",
    "    while not is_valid_eval:\n",
    "        print('Invalid evaluation, reevaluating...')\n",
    "        fresheval = call_fresheval(\n",
    "            model_name,\n",
    "            prefix,\n",
    "            question,\n",
    "            response,\n",
    "            correct_answers,\n",
    "            evaluation,\n",
    "        )\n",
    "        is_valid_eval, eval = extract_ratings(fresheval)\n",
    "        if is_valid_eval:\n",
    "            print('Done')\n",
    "    freshevals.append({'rating': eval['rating'], 'explanation': fresheval})\n",
    "\n",
    "df_eval = pd.DataFrame(freshevals)\n",
    "\n",
    "# 保存结果到本地CSV文件\n",
    "df_eval.to_csv(output_csv_file, header=True, index=True)\n",
    "print(f\"评估结果已保存到 {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HCdw8F-MZZlO",
    "outputId": "ae1b6659-ec51-46c9-de2c-c2d32da89235"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>evaluation: correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>This is a valid question. The response provide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>This is a valid question. The primary answer p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>This is a valid question. The primary answer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rating                                        explanation\n",
       "0  FALSE  This is a valid question. The primary answer i...\n",
       "1   TRUE  This is a valid question. The primary answer i...\n",
       "2   TRUE  This is a valid question. The primary answer i...\n",
       "3   TRUE                                evaluation: correct\n",
       "4  FALSE  This is a valid question. The primary answer i...\n",
       "5   TRUE  This is a valid question. The response provide...\n",
       "6  FALSE  This is a valid question. The primary answer p...\n",
       "7   TRUE  This is a valid question. The primary answer i...\n",
       "8  FALSE  This is a valid question. The primary answer i...\n",
       "9  FALSE  The question contains a false premise. The res..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[:num_evals]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
